# Mental Health Survey — Classification (Yes/No)

A reproducible data science project built from the provided Jupyter notebook **`Mental Health.ipynb`**.
The project explores a mental health survey dataset and predicts a binary outcome (“Yes”/“No”) using several classifiers.
It includes data cleaning, feature engineering, model training/tuning, and evaluation (accuracy, ROC-AUC, confusion matrix, and classification reports).

## TL;DR
- **Goal:** Predict a binary mental-health-related target (“Yes/No”) from survey responses.
- **Data:** `Mental_Survey.csv` (plus intermediate `results.csv`, if generated).
- **Top baseline model (from notebook outputs):** XGBoost around ~0.77 accuracy (ROC-AUC shown around ~0.80 on some baselines).  
  Other baselines include Logistic Regression (~0.70 acc), KNN (~0.75), Decision Tree (~0.75), SVC, GaussianNB, AdaBoost.
- **How to run:** one-command setup + notebook.

> Note: exact scores may vary by environment, random seed, and preprocessing details. See the notebook for the full outputs.

---

## Project Structure
```
.
├─ Mental Health.ipynb        # main analysis + modeling
├─ Mental_Survey.csv          # source dataset (expected path if available)
├─ results.csv                # optional, generated by notebook
├─ README.md                  # this file
├─ requirements.txt           # Python dependencies
└─ .gitignore
```

## Methods
### 1) Data Cleaning
- Remove invalid ages (e.g., `< 0` or `> 100`) to avoid skewing models.
- Handle missing values and inconsistent categories (see notebook cells).
- Sanity checks on categorical fields (domain-specific encoding).

### 2) Feature Engineering
- **Encoding:** One-hot encoding / LabelEncoder for categorical variables.
- **Scaling:** MinMaxScaler used for numeric fields where appropriate.
- **Train/Test Split:** `train_test_split` with a fixed `random_state` (see notebook).

### 3) Modeling
Implemented baselines and tuned variants (as present in the notebook):
- Logistic Regression
- K-Nearest Neighbors (KNN)
- Decision Tree
- Support Vector Classifier (SVC)
- Gaussian Naive Bayes
- AdaBoost
- **XGBoost (XGBClassifier)**
- (Optional section) **Neural Networks** via TensorFlow/Keras

**Hyperparameter search:** `RandomizedSearchCV` and/or `GridSearchCV` where included.

### 4) Evaluation
- **Metrics:** Accuracy, ROC-AUC, sensitivity/specificity at thresholds, confusion matrix, and `classification_report`.
- **ROC curves** and threshold analysis to understand trade-offs.
- Reported indicative results from the notebook:
  - Logistic Regression: ~0.70 accuracy, ROC-AUC ~0.75
  - KNN / Decision Tree: ~0.74–0.75 accuracy
  - **XGBoost:** ~0.77 accuracy, ROC-AUC often higher vs. linear baselines

> The exact numbers come from the notebook outputs; rerunning may produce slightly different results.

## How to Run

### Option A) Run the Notebook
```bash
# 1) Create and activate a virtual environment (example: venv)
python -m venv .venv
# Windows: .venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

# 2) Install dependencies
pip install -r requirements.txt

# 3) Launch Jupyter
python -m jupyter lab
# or
python -m jupyter notebook

# 4) Open `Mental Health.ipynb` and run all cells
```

### Option B) Convert to a Python script (optional)
```bash
jupyter nbconvert --to script "Mental Health.ipynb"
python "Mental Health.py"
```

## Reproducibility Notes
- Set a consistent random seed where applicable.
- Ensure the dataset path matches your local file system.
- Python and library versions can affect results; see `requirements.txt` for suggested packages.

## Dataset
- Primary file referenced in the notebook: **`Mental_Survey.csv`**.  
  Put it in the project root or update the path in the notebook cell where it’s loaded.
- Intermediate artifacts: **`results.csv`** may be generated during EDA/modeling.

## Environment
See `requirements.txt`. The notebook uses common DS/ML libraries:
- pandas, numpy, matplotlib, seaborn
- scikit-learn
- xgboost
- (optional) tensorflow / keras
- statsmodels, scipy, mlxtend (if you use feature selection/plots from these)

## License
Add a license of your choice (e.g., MIT). If you publish this publicly, include a `LICENSE` file.

## Acknowledgments
- Thanks to the maintainers of scikit-learn, xgboost, and the broader Python data science ecosystem.
- If your dataset comes from a public source (e.g., Kaggle/academic survey), add proper attribution here.

---

### Badges (Optional)
You can customize and add badges:
- ![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
- ![scikit--learn](https://img.shields.io/badge/scikit--learn-1.x-orange)
- ![xgboost](https://img.shields.io/badge/XGBoost-1.x-green)
